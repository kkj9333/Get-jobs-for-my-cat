# Day 16 Quantifying the Knowledge in a DNN to Explain Knowledge Distillation for Classification
## 概况
本文提出了一种基于信息论的新视角，通过中间层编码的知识点的量化来解释知识蒸馏成功的原因。通过分析深度神经网络中编码的知识点的数量和质量来衡量特征表示的可靠性。作者通过三种不同的量化知识点的方法来验证以下三种提出的假说：
1. 知识蒸馏使得网络编码比从头开始学习更多知识点; 
2. 知识蒸馏使网络更可能同时学习不同的知识点，从而产生更好的性能; 
3. 知识蒸馏通常比从头开始学习更稳定优化方向。

作者的研究结果表明了这三个假设的正确性，并且通过多个数据集和分类任务的实验验证了这些假设。
