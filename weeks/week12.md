# **Day 10 An Efficient and Robust Cloud-based DeepLearning with Knowledge Distillation**

## 论文摘要概述
这篇论文在云边知识蒸馏领域提出了一种新的神经元流形蒸馏（NMD）方法，其中学生模型模拟教师的输出分布，并学习教师模型的特征几何。此外，为了进一步提高基于云的学习系统的可靠性，我们提出了一种自信的预测机制来校准模型的预测，并在多个数据集上证明了其有效性。
## 论文框架模型
论文提出的框架模型如下所示:<br>
![image](https://user-images.githubusercontent.com/51207072/226775333-3b4a23c5-5a3d-4efb-bcd2-6e2abe80c484.png)<br>
浅蓝色的教师网络是一个标准的深度残差网络ResNet110，奶油色的学生网络是一个浅层网络ResNet20，
两个网络都有3个**残余块**（residual block），但基本块的数量不同。ResNet110在每个残差块中有18个基本块，但ResNet20只有3个。知识蒸馏发生在每个**残余块**之后。
## 蒸馏方法NMD
在假定相同批次处理数据的知识蒸馏训练中，有教师特征集ft∈Ft和学生特征集fs∈Fs，对于预定义特征提取函数ψ(.)可以提取为ψ(ft)和ψ(fs)（注意是将高维特征映射到一个保留关键特征特征的相同维度的低维空间），于是有每对的l2距离：![image](https://user-images.githubusercontent.com/51207072/226776901-037d526c-7ae6-4c79-95c7-41fa53cca95d.png)，于是根据以前的蒸馏方法以及新的这个特征函数有新的损失函数：<br>
![image](https://user-images.githubusercontent.com/51207072/226776992-c11ad181-cb8b-4810-b439-08c4cb986d57.png)<br>
α、β、γi（_个人见解这里的yi是对应特征提取函数的l2距离前的_）是控制每个分量权重的超参数。超参数λ的选择非常重要的。对于特征f，浅层特征则选择一个相对较大的λ值，深度低层特征选择一个较小的λ值。
### 线性特征流形
流形学习的观点是认为，我们所能观察到的数据实际上是由一个低维流形映射到高维空间上的。由于数据内部特征的限制，一些高维中的数据会产生维度上的冗余，实际上只需要比较低的维度就能唯一地表示。
也就是说，流形可以作为一种数据降维的方式；另外流形能够刻画数据的本质。也就是说。既然学习到了“将数据从高维空间降维到低维空间，还能不损失信息”的映射，那这个映射能够输入原始数据，输出数据更本质的特征。<br>
<br>
假定一个d维线性特征流形M ⊂ R^d 嵌入在 R^m中，可定义一个构造函数h：
![image](https://user-images.githubusercontent.com/51207072/226779658-cfad7a3e-97cf-43b3-bb73-8e01507b3492.png)<br>
给定N个从DNN学习到的特征fi，可能是采样来自d维线性特征流，并且带有噪音：
![image](https://user-images.githubusercontent.com/51207072/226780118-33cd467c-c730-4bfd-ad60-f8fce1dff20b.png)<br>
vi是第i个特征fi的特征流形，ϵi是误差。目标是根据fi估计未知的低维特征向量vi，即想用更低维的vi来表示学习特征。<br>
![image](https://user-images.githubusercontent.com/51207072/226789547-949ebe4f-d07a-4fa8-9971-8ca700e5f89c.png)
F = [f1，f2···，fN ]包括所有N特性，我们的目标是找到c，A和V最小化近似误差E V = [v1，v2，···，vN]，E=[ϵ1，ϵ2，···ϵN],e是所有的N维列向量。
关于这里论证特征流形的举例二维的例子这里不再介绍,这里假设了c = f ¯ = F e/N（F的平均值），并采用SVD分解的方法来求AV的表达式，结论如下：<br>
![image](https://user-images.githubusercontent.com/51207072/226790110-0e90594c-ea88-4732-bde3-02a09b33878c.png)<br>
![image](https://user-images.githubusercontent.com/51207072/226790132-466bd384-eabc-497a-bed6-fff9c6bacb12.png)<br>

### 深层特征的特征流形
深度学习中特征流形更加复杂，非线性化。非线性流形学习的关键困难是无组织的降维数据。论文认为对非线性深度特征的流形学习的目标是在不明确知道h（·）的情况下，从相应的特征fi或h（vi）重构vi。

## 总结

# **Day 11 Catastrophic Interference in Reinforcement Learning: A Solution Based on Context Division and Knowledge Distillation**

## 论文摘要概述
强化学习（RL）代理能够直接从连续的环境中学习有能力的控制策略，但是在实际情况中中，训练数据是时间相关的和不平稳的，一般的RL范式无法保证相同和独立分布，这就影响了RL学习到的概率分布的有效性和预测能力。论文提出干扰感知的深度q学习（IQ）来减轻单任务深度RL中的灾难性干扰，具体采用在线聚类来实现动态上下文划分，以及多头网络和知识蒸馏正则化术语，以保存学习上下文的策略。
## 强化学习干扰问题
强化学习的RL问题实质上是是在神经网络中观察到的一种现象，未来的训练可能会覆盖和干扰之前学习到的良好策略，并显著降低之前任务的表现，而强化学习的数据不平稳放大了这种现象。如下图所示：<br>
![image](https://user-images.githubusercontent.com/51207072/226833719-1985d9e2-0dd0-4e1b-802d-1a7f7d16d5a0.png)<br>
- (a)是单任务RL中的灾难性干扰的说明。学习过程中数据分布的漂移，其中P1-P3是不同的数据分布，➀-➁代表分布转换。在学习过程中，代理会经历以下数据分布转换：序号表示迁移顺序，箭头表示迁移方向。
- (b)DNNs的稳定性-可塑性权衡。Sharing：这两个学习阶段(Old learning at P1 and Current learning at P2)训练相同的模型。transfer：当前的学习阶段继续对来自旧的学习阶段的模型进行训练。干扰：模型在P2上进行训练后，右侧网络中绿色的权值发生变化，影响了模型在P1上的性能。
- (c)学习曲线中，实线对应于(a)中的数据分布转换，虚线表示训练性能。在t3之前，数据分布逐渐从P1漂移到P2和P3。当模型适合于P3时，P1和P2上的学习策略就会受到干扰，当代理再次遇到来自P1的状态时，就会导致灾难性的性能下降。因此，该模型需要在P1上进行再训练（在T3→T4时间段内）。同样的问题也发生在时间段T4→T5中。
一旦训练数据的分布出现明显的漂移，就有可能发生灾难性的干扰和连锁反应，导致训练性能的突然恶化.
### 目前处理灾难性干扰的方案
- 经验重放，通常对关键参数（例如，重放缓冲区容量）表现出高水平的敏感性，将系统探索环境得到的数据储存起来，然后随机采样样本更新深度神经网络的参数，并且通常需要保持一个大的临时存储内存。但是违背了当前最先进的算法的要求，即数据应该接近策略上的分布。
- 局部优化，提倡对具有特定分布的数据进行局部网络更新，而不是进行全局泛化，以减少不同数据分布之间的表示重叠。主要的问题是，一些方法在不同分布的数据之间的模型传输能力有限，或需要预训练，可能不适用于在线设置。
### 本文工作重点
本文主要讨论了单任务RL中由状态分布漂移引起的灾难性干扰问题。
- 提出一个干扰感知方案低缓冲大小灵敏度称为干扰感知深度Q-learing（IQ）
- 在线估计值函数为每个状态分布通过最小化原始损失函数的加权和RL算法和正规化术语关于不同状态之间的干扰
本文是通过如下方法来实现这些的：
- 提出了一种基于在线聚类（K-mean）的上下文划分策略，将状态空间划分为一系列独立的上下文，来减轻减轻模型训练过程中不同状态分布之间的干扰。
- 通过在多任务学习中常用的具有多个输出头的神经网络来参数化值函数，其中每个输出头专门针对特定的上下文，并且特征提取器在所有上下文中共享。
- 我们在目标函数估计值函数中应用知识蒸馏作为正则化项，可以保留学习到的策略，而RL代理在当前策略的指导下进行训练，以进一步避免共享的低级表示造成的干扰。
## 相关工作

### 上下文检测和识别
CL学习任务相关性需要界定任务边界，大多数多任务CL方法依赖于定义的任务边界，通常在一系列已知标签或边界的任务上进行训练。
<br>现有的上下文检测方法通常利用统计数据或贝叶斯推理来识别任务边界。
一方面，一些方法通过发现变化点模式状态奖励元组，跟踪短期和长期移动平均奖励，或将游戏分离成使用未折现的累积的游戏分数作为任务的上下文来响应变化的分布，这些方法可以敏捷地响应上下文或任务之间发生突然变化的场景，但是对平滑过渡的上下文并不敏感；另一方面，一些更雄心勃勃的方法试图直接从环境互动的历史中学习对未观察到的环境状态的信念，然而在应用于CL问题之前，它们都需要对完整的数据进行预训练，

## 一些定义&前置知识

### Huber loss
![image](https://user-images.githubusercontent.com/51207072/227679124-0a3012c2-92ea-46fe-ab68-325f368c54d7.png)
### 知识蒸馏
蒸馏的概念最初用于将“知识”从一个复杂的网络集合转移到一个相对更简单的网络中，以降低模型的复杂性和便于部署。知识蒸馏可以很好地鼓励一个网络的输出接近另一个网络的输出。

### 表征学习（representation learning）
针对不同类型的数据（text，image，video），不同的表示可能会导致有效信息的缺失或是曝露，这决定了算法能否有效地解决问题。
表征学习的目的是对复杂的原始数据化繁为简，把原始数据的无效的或者冗余的信息剔除，把有效信息进行提炼，形成特征（feature）。特征提取可以人为地手工处理，也可以借助特定的算法自动提取。Roughly Speaking， 前者为特征工程，后者为表征学习（Representation Learning）。

### MLP
最简单且原汁原味的神经网络则是多层感知器（Multi-Layer Perception ，MLP）,最典型的MLP包括包括三层：输入层、隐层和输出层，MLP神经网络不同层之间是全连接的（全连接的意思就是：上一层的任何一个神经元与下一层的所有神经元都有连接）。
![image](https://user-images.githubusercontent.com/51207072/227707871-72e285f4-fe65-46dc-ae6a-874cc710ffe2.png)

### RL范式（RL Paradigm）
RL问题被认为是一个马尔可夫决策过程（MDP），被定义为一个元组：![image](https://user-images.githubusercontent.com/51207072/227400539-d1f33b61-b514-4540-8042-d4c91ef4b6b9.png)
，S是状态集，A是动作集；P：S×A×S→[0,1]是环境转移概率函数，R： S×A×S→R是奖励函数，γ∈[0,1]是折扣因子。<br>
根据定义，在每个时间步t∈N，代理（_the agent_）在采取At行动后以概率P（St+1|St，At）从St移动到St+1，并获得奖励R（St，At）。
在这个基础上定义了基于价值的RL模型的优化目标：
### RL优化目标（RL Optimization Objective）
基于价值的RL的优化目标是学习一个内部参数为![image](https://user-images.githubusercontent.com/51207072/227401926-9db40931-ed3a-4492-bcb5-b39c9eec2253.png)
的策略π(a|s)，使每个(s, a)在时间上的预期长期折现收益最大化、 也被称为价值函数：<br>
![image](https://user-images.githubusercontent.com/51207072/227404186-804ca206-089b-48a2-99e6-5b9f6d2d19da.png)
在这里，期望值超过了使用P生成历史记录并决定从π到代理生命周期结束的操作的过程。这个优化目标不仅关注当前状态，还关注状态的完整预期未来分布。因此，**有可能**（作者yy）克服RL在非平稳数据分布上的灾难性干扰。<br>
然而，最近在RL方面的大部分工作都是在所谓的情景环境中，它优化了情景（_episodic_）RL的目标：
### 情景式RL优化目标（Episodic RL Optimization Objective）
给定某个未来期（future horizon）H，找到一个策略π(a|s)，优化预期折现收益：<br>
![image](https://user-images.githubusercontent.com/51207072/227404906-ad6b3dbf-00e0-4ebe-ab9c-6e9644025e8e.png)<br>
在这里，为了确保优化的可行性和易于实施，目标只在未来期H范围内进行优化，直到当前阶段结束。（作者假定）很明显，这个情景目标偏向于当前的情景分布，而忽略了在代理的一生中可能更重要的未来情景分布。这样的优化目标插入非平稳的RL设置中会导致偏置优化，这可能会导致灾难性的干扰效应。<br>
### 价值函数近似方法
对于大规模域，价值函数通常与参数函数类的成员近似，例如参数![image](https://user-images.githubusercontent.com/51207072/227407791-548270b4-87ba-4ff9-b49b-a72ecf551b83.png)，表示为Q(s,a;θ)的神经网络，使用以(s,a,r,s')形式的经验样本在线拟合。这种经验通常被收集到一个缓冲区B，然后随机抽取批次，以形成对损失（_loss_）的随机估计:<br>
![image](https://user-images.githubusercontent.com/51207072/227408249-b968e186-42ac-46ca-b43d-fb81337ce9c7.png)<br>
其中，L： ![image](https://user-images.githubusercontent.com/51207072/227410623-ee744878-5e81-40fd-b866-b7d2e9764575.png)是代理的损失函数，μ∈P (B)是定义其采样策略的分布。一般来说，用于计算目标Q(s',a';θ-)的参数θ−是用于动作选择的优先副本（作为DQN的设置）。
### 非平稳（ Nonstationary）
它是一个状态或概率分布随时间变化的过程。
### 干扰（Interference）
它是一种基于梯度的两个进程之间的影响，其目标是J1和J2，并共享参数θ。 干扰的特征通常是在一阶中由它们的梯度的内积来描述的：<br>
![image](https://user-images.githubusercontent.com/51207072/227413877-ea20ef72-0b5c-4b6b-9979-82956d5ccb3d.png)<br>
当梯度更新时（![image](https://user-images.githubusercontent.com/51207072/227414530-bcb0dae3-9331-4b79-a4e3-22e9d9c65443.png)）：<br>
- ρ > 0，转移，视为建设性的
- ρ < 0，干扰，视为破坏性的
### 灾难性干扰（Catastrophic Interference）
在神经网络训练中观察到的一种现象，学习一个新任务会显著降低先前任务的表现。
## 问题陈述
单任务RL内的干扰可以通过当前策略下模型更新前后的**TD误差**的差异来近似衡量，称为近似期望干扰（AEI）：<br>
![image](https://user-images.githubusercontent.com/51207072/227415288-7c2c6d4f-0335-4627-bfe7-99b912caee99.png)<br>
其中dˆ为当前策略下(s,a,r,s')的分布，TD误差是δ(s,a,r,s';θ)=r+![image](https://user-images.githubusercontent.com/51207072/227416080-fb12ea70-361a-47ea-ae66-11522fff89c0.png)-Q(s,a;θ)。
### 单任务RL训练中干扰和代理性能的相互作用
![image](https://user-images.githubusercontent.com/51207072/227427655-73423293-114f-4952-8e40-fed97b0d8aa7.png)
如图是OpenAI基线3中实现的DQN进行的实验，当AEI开始增加时，性能开始振荡，一般来说，在干扰增加的情况下，代理的性能倾向于显著下降。这一结果为干扰与单任务RL模型的稳定性密切相关提供了直接的证据。
### 本文解决问题表述
提出一种新的、有效的单任务RL训练方案，以减少训练过程中的灾难性干扰和性能振荡，同时提高稳定性和整体性能。
## 解决方案框架
本文的方案IQ由三个主要组成部分组成，共同优化以减轻单任务RL中的灾难性干扰：上下文划分、知识蒸馏和多头神经网络（采用了一个具有共享表示层的多头神经网络来为特定分布的价值函数设置参数）的协同训练。<br>
基于IQ，进一步提出了基于随机编码器（IQ-RE）的干扰感知深度q学习方法，用于高维状态空间的有效上下文化。
本文提出的IQ方案可以纳入任何现有的基于值的RL方法，以训练单任务RL的分段q函数。如图所示，该框架由三部分组成：
- 上下文划分，包括状态分配和质心更新（centroid update），自适应上下文划分通过顺序K-means在线聚类实现；
- 知识蒸馏-将知识蒸馏损失（LD）纳入目标函数（Lori），以避免共享特征提取器导致的上下文间干扰；
- 与多头神经网络联合优化，目的是通过联合优化损失（L = Lori +λLD）估计每个[s,a,w(s)]的值。
将神经网络的表示模块称为“RL编码器”。综上所述，我们的方法可以通过解耦不同分布状态之间的相关性和有意地保留学习到的策略来提高性能。<br>
![image](https://user-images.githubusercontent.com/51207072/227679852-6404b50c-f803-4faf-bf53-e2a9437487b8.png)<br>
q函数的权值集合记为θ = {θS,θT,θF}，其中θS是一组共享参数，而θT和θF都是特定于上下文的参数： θT是对应于当前输入状态s的上下文，而θF是对应于其他输入状态s的上下文。本文以以IQ和基本的RL算法DQN的结合作为一个示例。
## 步骤
### 1.上下文划分
在MDPs中，状态（或“观察”）代表了关于环境的最全面的信息。为了理解不同分布的状态，为状态空间中相互接近的状态集合定义了一个变量ω，称为上下文<br>
![image](https://github.com/kkj9333/Get-jobs-for-my-cat/blob/f2a04ec96fa1301e1d591bf56035db4cd72fb2b0/weeks/asserts/image.png)
<br>
其中Ω是一个有限的上下文集合，k是上下文的数量。对于任意MDP将其状态空间划分为k个上下文，每个上下文中的所有状态都遵循近似相同的分布，以解耦状态们与分布漂移的相关性，我们将一个上下文ωi与每个集合Si关联起来，因此对于s∈Si，ω(s) = ωi，其中ω(s)可以被认为是状态s的函数。<br>
由于RL学习中探索学习特性，代理（the agent）在寻找最优策略时通常不会体验到环境的所有可能状态。基于这个事实，在IQ中，我们只对训练中所经历的状态进行上下文划分。本文采用了顺序的K-means聚类来实现自适应的上下文检测（见补充材料的附录A-A）。<br>
在如图框架中，k个质心C = {c1，c2，...，ck }在整个状态空间中被随机初始化。在随后的每个时间步长t中，我们对从环境接收到的每个传入状态执行状态分配和质心更新步骤（作者建议执行这两个步骤前在不同的维度上对状态进行规范化，以获得更合理的上下文划分结果。），并将其对应的转换（_transition_）{st,ω(st),at,rt,st+1,ω(st+1)}存储到重放缓冲区B（图中看起来像R）中。因此，在训练阶段，我们从B随机抽取一批转换，同时训练共享特征提取器![image](https://user-images.githubusercontent.com/51207072/227679940-ee5c766e-6979-44f6-88ad-cb75be9ee382.png)<br>
和输入状态对应的特定输出头![image](https://user-images.githubusercontent.com/51207072/227679953-abacc0a6-a933-4acb-b338-8602b68e6092.png)<br>
，同时对其他输出头进行微调（_Fine Tune_），以避免对学习策略的干扰。由于我们将每个状态的上下文标签存储在重播缓冲区中，因此在每个更新步骤中都不需要额外的状态分配（可以看到只需要更新w(s)）。（我们只需要为每个状态执行一次状态分配??）<br>
也可以基于初始状态分布进行上下文划分，相比之下，我们表明，对所有经历过的状态的分区在训练可以产生更准确和有效的上下文划分结果，轨迹从初始状态在不同的上下文中有很高的可能性重叠在后续的时间步骤（见补充材料的附录A-B）。<br>

- 上下文干扰（Interference Among Contexts）。上下文划分法得到的上下文之间的干扰，具体来说，当代理在其他上下文下学习时，我们在游戏的不同上下文中测量TD错误的Huber损失，然后记录代理学习前后损失的相对变化。如上图结果表明，在任何情境下的长期训练都可能导致在所有其他情境下的负泛化，即使是在这样简单的RL任务CartPole-v0中（补充材料的附录A-C中运行结果也支持这个结论）。
![image](https://user-images.githubusercontent.com/51207072/227679972-b6bfe2cd-d7b4-4396-aee6-66bd0a5eb934.png)
通过在CartPole-v0上训练400k环境步骤时，聚类所有有经验过的状态来测量上下文之间的干扰（k = 3）。当代理在特定上下文上进行训练时，我们记录了所有上下文的Huber损失的相对变化。显然，对特定环境的培训通常会减少对其本身的损失，并增加对所有其他环境的损失。
- 计算复杂性（Computational Complexity）。假设有k个上下文的d维环境，我们提出的上下文划分模块处理T个环境步骤的时间和空间复杂度分别为O（Tkd）和O（kd）。
### 2.知识蒸馏
共享的低级表示（low-level representation）可能导致新环境中的学习干扰之前的学习结果，从而导致灾难性的干扰。知识蒸馏可以很好地鼓励一个网络的输出接近另一个网络的输出，在IQ中，我们使用它作为价值函数估计中的一个正则化项，以保留先前学习到的信息。
当在特定的上下文上训练模型时，我们需要考虑损失函数的两个方面：
- 当前训练上下文的一般损失，由Lori表示。鼓励模型适应当前的上下文，以确保可塑性。
- 其他上下文的蒸馏损失，由LD表示。后者鼓励模型保持对其他上下文的记忆，防止干扰。
为了将IQ纳入DQN框架，我们改写了
![image](https://user-images.githubusercontent.com/51207072/227408249-b968e186-42ac-46ca-b43d-fb81337ce9c7.png)<br>
中DQN的原始损失函数，以上下文变量ω为：
##### 原始损失函数
![image](https://user-images.githubusercontent.com/51207072/227683406-d474449c-1e0c-4916-9486-4bfa063101a3.png)<br>
![image](https://user-images.githubusercontent.com/51207072/227683531-c23623ee-f4b8-4120-be9a-e20ebe1bb645.png)<br>
是Q(s,a,w(s);θS,θT)的估计目标值，μ为样本的分布，即![image](https://user-images.githubusercontent.com/51207072/227685247-7ea6836d-a1be-4dd2-98ab-294c57308fb3.png)
，L为Huber损失<br>
对于环境所包含的每个其他上下文，我们期望每对(s,a)的输出值接近于原始网络的记录输出。
1. 我们将当前更新步骤前学习到的Q函数视为教师网络，表示为![image](https://user-images.githubusercontent.com/51207072/227687515-efdbf5a4-e3e9-4e30-a048-cd8de7067585.png)=![image](https://user-images.githubusercontent.com/51207072/227687629-964f217b-15d7-4115-8030-da6edcee3fc8.png)
2. 当前网络训练为学生网络（请注意主要是针对其他上下文被此次更新影响到的输出，要模拟到之前的各自上下文的输出），表示为![image](https://user-images.githubusercontent.com/51207072/227687756-c409905a-1a4d-43b4-8366-a11356121d81.png)，其中当前ωi∈Ω且不包含当前的上下文ω(s)。因此，蒸馏损失被定义为：
##### 蒸馏损失函数
![image](https://user-images.githubusercontent.com/51207072/227688110-eb907623-55d1-427f-8b74-ead80092dc65.png)<br>其中![image](https://user-images.githubusercontent.com/51207072/227688404-669804b0-eb17-480c-8291-79530a3a6782.png)是与上下文ωi对应的输出头的蒸馏损失函数。
### 3.联合优化程序（Joint Optimization Procedure）
为了优化出一个可以指导代理在每个上下文上做出适当的决策，而不受到灾难性干扰的不利影响的Q函数，我们结合[原始损失](https://github.com/kkj9333/Get-jobs-for-my-cat/blob/main/weeks/week12.md#原始损失函数)和[蒸馏损失](https://github.com/kkj9333/Get-jobs-for-my-cat/blob/main/weeks/week12.md#蒸馏损失函数)来形成一个联合优化框架。即，我们通过以下优化目标（_optimization objective_）来解决灾难性干扰问题：<br>
##### 联合损失函数
![image](https://user-images.githubusercontent.com/51207072/227689463-64afb9fe-cfcc-4918-928f-a0e4a70eb8bc.png)<br>
λ∈[0,1]是控制神经网络稳定性和可塑性之间权衡的系数（这个是超参数？？）。<br>
在算法1中描述了完整的过程。<br>
![image](https://user-images.githubusercontent.com/51207072/227689936-15b2b79d-3939-49f6-899d-a51e15d4406a.png)<br>
该方法
1. 在不需要额外数据的情况下并行执行上下文分割，并行训练过程。
2. 在网络训练中，为了降低与目标的相关性，确保模型训练的稳定性，目标网络参数θ−每隔C步仅通过q网络参数θ进行更新，并在个体更新之间保持固定，如DQN所示。
3. 同样，采用了固定目标上下文质心(Cˆ)，以避免不断更新的上下文质心(C)所带来的少量状态分配步骤的不稳定性。
4. 为了简化模型的实现，我们将目标上下文质心的更新频率设置为与目标网络一致。
### 4.高维状态空间中的随机编码器（Random Encoders for High-Dimensional State Space）
对于高维状态空间，我们建议使用随机编码器进行有效的上下文划分，它可以将高维输入映射到低维表示空间，克服了“维数的诅咒”。虽然原始的RL模型已经包含了一个编码器模块，但它会不断更新，并且在其表示空间中直接执行聚类可能会给上下文划分带来额外的不稳定性。因此，基于IQ，我们利用一个专用的随机编码器模块进行降维。<br>
![image](https://user-images.githubusercontent.com/51207072/227690102-0cc2412f-dba6-476b-9977-17bfc708b3a3.png)<br>
IQ-RE:上下文划分是在一个随机编码器的低维表示空间中执行的。一个单独的RL编码器用于处理MLP层来估计价值函数。
其中随机编码器fθre的结构与底层的RL编码器一致，但其参数θre在整个训练过程中都是随机初始化和固定的。<br>
使用随机编码器的主要动机来自于这样的观察：随机编码器的表示空间中的距离足以在没有任何表征学习（_representation learning_）的情况下找到相似的状态，也就是说，随机编码器的表示空间可以在没有任何表征学习（_representation learning_）的情况下有效地捕捉状态间的相似性信息（见补充材料的附录B-A）。在补充材料的附录B-B中，用随机编码器和底层RL训练的编码器进行的额外的IQ比较实验进一步强调了 随机编码器的优越性。

## 实验评估
评估实验数据集在四个经典控制（_classic control_），六个高维复杂的雅达利游戏（high-dimensional complex Atari games），前者易于理解和相对简单，适合突出显示机制，后者进一步证明论文方案在高维复杂任务上的可伸缩性。
### 实现
1. 网络结构：对于四个经典的控制任务，我们采用一个全连接层作为特征提取器，而一个全连接层作为多头动作评分器（multihead action scorer），遵循OpenAI基线中这类任务的网络配置。对于6款雅达利游戏，我们使用某个经典的卷积神经网络进行特征提取，并使用两个完全连接的层作为多头动作评分器。更多细节见补充材料的附录C。
2. 参数设置：在IQ中，有两个关键参数：λ和k。为了简化参数设置，我们根据所有实验中的探索比例设置λ：λ=1−，因为在训练中它们之间存在反比关系。在早期训练中，接近于1，模型通常不准确，干扰较小，且较小的λ（接近于0）可以促进模型的可塑性构建。然后在随后的训练中，逐渐接近0，模型学习到了更多有用的信息，同时也有可能发生干扰。因此，需要平稳地增加λ，以确保可塑性，同时避免干扰。同时，我们将所有经典控制任务的k设置为3，将所有雅达利游戏设置为4。在IQ-RE中，我们将额外的参数d设置为50，如在中一样，这已被证明是既有效和有效的。其他参数设置见补充材料的附录C。对于经典的控制任务，我们使用的平均10k时间步长 for CartPole-v0 and Pendulum-v0，以及20k时间步长 for CartPole-v1和Acobot-v1来评估训练性能。对于雅达利游戏，性能评估的时间步长范围为200k。所有报告的实验结果都是在5次独立运行后的平均发作结果。
### 基线标准
与DQN&Rainbow（属于 uderlying RL method）， SRNN，DSOM，TCNN的单任务RL基线方法进行了比较（并非同时在各个方面）。本实验方案是基于DQN进行的。

### 评估指标
按照之前的研究的惯例，我们使用平均训练集返回RT来评估训练期间我们的方法：<br>
![image](https://user-images.githubusercontent.com/51207072/227705006-4ab6b9a0-4178-469d-a2b8-ea471b365f7d.png)<br>
式中，M为每个评价周期内经历的事件次数，Ji为事件i（episode i）的总时间步长，Rij为事件i（episode i）在时间步j时获得的奖励。
### 实验结果
#### 经典控制
下图显示了具有三个重放缓冲容量级别的每个任务在训练过程中平均情景返回的学习曲线，实线和阴影区域分别表示5次运行中奖励的平均值和标准差。(a) N = 50 000.(b) N = 100.(c) N = 1.：<br>
![image](https://user-images.githubusercontent.com/51207072/227705198-5f90d055-f195-4b3a-8f61-215312c43a75.png)<br>
表1报告了（基于上图5次运行的）相应曲线中获得的最高累积返回的数值结果。<br>
![image](https://user-images.githubusercontent.com/51207072/227705754-d1adfbe5-2a14-435a-b8e9-de660435fc36.png)<br>
一般来说，在稳定性和最大累积奖励方面，IQ都明显优于所有基线，特别是当重放缓冲容量很小时（例如，N = 100），甚至没有经验重放（即，N = 1）。在大多数任务中，即使没有任何经验回放，IQ也能达到接近最佳的表现和良好的稳定性。我们还对所有基线的干扰程度进行了实验比较，从中我们可以进一步证实，IQ可以大大减少基础RL代理在学习过程中遇到的负干扰。<br>
学习曲线图中还有以下要素：
1. DQN、SRNN和TCNN代理对重放缓冲区容量表现出较高的敏感性。它们通常在大缓冲区下表现良好（CartPole-v1除外），但当缓冲区容量降低时，它们的性能会显著下降。产生这种现象的原因是，DQN主要依赖于经验重放来获得大约的i.i.d.训练数据，避免训练中可能出现的干扰，这在重放缓冲容量较小时无法保证。由于SRNN只是基于DQN损失重构了约束项，而TCNN只增加了DQN的输入维数，因此这两种技术都只能在一定程度上减轻干扰。
2. DSOM的整体性能优于上述三个基线，因为它优化了数据特定于分布的表示模块，以避免由共享表示层引起的干扰。然而，由于DSOM共享相同的输出层，因此它在大多数任务上仍然受到干扰。
3. 相比之下，IQ具有一个共享的表示模块和多个特定于数据分布的输出头，并使用知识蒸馏技术来防止共享表示层造成的干扰，实现了明显优于基线的性能。请注意，在某些情况下（例如，CartPole-v1设置），在训练的早期阶段，IQ学习得比基线要慢。一种可能的解释是，IQ以完全在线的方式学习上下文划分，而当时的划分可能不够准确，但随着训练的进展，它可以很快超过基线。
#### 雅达利游戏（高维编码）
此外，为了展示我们的方法的可伸缩性和灵活性，还在6款雅达利游戏上对比了DQNs和IQ-RE的结果（请注意），学习曲线图如下，值得注意的是，(a)中高速公路的绿线和红线与(b)中高速公路的紫色、橙色和粉色线是重叠的。(a) N = 1 000 000.(b) N = 10 000.：<br>
![image](https://user-images.githubusercontent.com/51207072/227706627-53f3f631-4cf3-4c4b-ae88-f849c7b56738.png)<br>
训练期间获得的最高累积回报见下表：<br>
![image](https://user-images.githubusercontent.com/51207072/227706725-957575ed-bc21-44f9-a88e-e7b1ac06e625.png)<br>
总的来说，对于高维图像输入，我们的IQ-RE方案可以显著提高底层RL算法的训练性能，而SRNN对这两种底层RL方法的贡献都很小。IQ-RE在12个任务中有7个显著优于DQN和SRNN，在其余5个任务上与DQN和SRNN相当。类似地，以Rainbow作为基本的RL方法，IQ-RE在12个任务中有8个优于基线。此外，如表所示，IQ-RE在大多数任务中的最高累积得分更高，只有4个案例表现略低于基线。值得注意的是，即使有很大内存（N = 1 000 000），IQ-RE仍然比基线有一定的优势。
#### 实验结论
综上所述，所提出的包含基于经验过状态聚类的上下文划分和在多头神经网络中的知识精馏的技术可以有效地消除单任务RL中数据漂移引起的灾难性干扰，同时减少了非策略RL对重放缓冲区容量的需求。此外，我们的方法（IQ-RE）利用一个固定的随机初始化编码器来表征低维表示空间中状态之间的相似性，可以用于高维环境的上下文分区。

### 分析
#### 消融研究（Ablation Study）
分别消除：
1. 基于在线聚类的自适应上下文划分，意味着在学习前使用原始状态空间的随机划分。
2. 蒸馏损失，即从[联合损失函数](https://github.com/kkj9333/Get-jobs-for-my-cat/blob/main/weeks/week12.md#联合损失函数)（即λ = 0)中去除[蒸馏损失函数](https://github.com/kkj9333/Get-jobs-for-my-cat/blob/main/weeks/week12.md#蒸馏损失函数)LD（θS、θF）。
3. 多头，意味着删除上下文分割模块，并使用单头输出优化神经网络（即k = 1）。这里，蒸馏项表示为输出头每次更新之前网络的蒸馏（貌似无意义了？）。
与基础DQN的实验对比结果如下：<br>
![image](https://user-images.githubusercontent.com/51207072/227708688-f9d3a364-08f5-4fcf-8586-c07dcc820914.png)

## 论文总结

在本文中，我们提出了一个有效的方案IQ来解决单任务RL中灾难性干扰的固有挑战。其核心思想是使用在线聚类技术将训练过程中经历的所有状态划分到一组上下文中，同时使用多头神经网络估计上下文特定的价值函数以及知识蒸馏损失，以减轻跨上下文的干扰。此外，我们还引入了一个随机的编码器（在IQ-RE中），以增强对高维复杂任务的上下文划分。
我们的方法可以有效地解耦不同分布状态之间的相关性，并可以很容易地合并到各种基于值的RL模型中。在几个基准测试上的实验表明，我们的方法可以显著地优于最先进的RL方法，并显著地降低了现有RL方法的内存需求。<br>
在未来，我们的目标是将我们的方法纳入基于策略的RL模型中，通过对策略应用权重或函数正则化来减少训练过程中的干扰。此外，我们将研究一个更具挑战性的设置，称为非平稳环境中的连续RL。这个设置是对现实世界场景的一个更现实的表示，包括动态上的突变或平滑过渡，甚至动态本身被打乱。




