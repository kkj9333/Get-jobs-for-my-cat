# **Day 10 An Efficient and Robust Cloud-based DeepLearning with Knowledge Distillation**

## 论文摘要概述
这篇论文在云边知识蒸馏领域提出了一种新的神经元流形蒸馏（NMD）方法，其中学生模型模拟教师的输出分布，并学习教师模型的特征几何。此外，为了进一步提高基于云的学习系统的可靠性，我们提出了一种自信的预测机制来校准模型的预测，并在多个数据集上证明了其有效性。
## 论文框架模型
论文提出的框架模型如下所示:<br>
![image](https://user-images.githubusercontent.com/51207072/226775333-3b4a23c5-5a3d-4efb-bcd2-6e2abe80c484.png)<br>
浅蓝色的教师网络是一个标准的深度残差网络ResNet110，奶油色的学生网络是一个浅层网络ResNet20，
两个网络都有3个**残余块**（residual block），但基本块的数量不同。ResNet110在每个残差块中有18个基本块，但ResNet20只有3个。知识蒸馏发生在每个**残余块**之后。
## 蒸馏方法NMD
在假定相同批次处理数据的知识蒸馏训练中，有教师特征集ft∈Ft和学生特征集fs∈Fs，对于预定义特征提取函数ψ(.)可以提取为ψ(ft)和ψ(fs)（注意是将高维特征映射到一个保留关键特征特征的相同维度的低维空间），于是有每对的l2距离：![image](https://user-images.githubusercontent.com/51207072/226776901-037d526c-7ae6-4c79-95c7-41fa53cca95d.png)，于是根据以前的蒸馏方法以及新的这个特征函数有新的损失函数：<br>
![image](https://user-images.githubusercontent.com/51207072/226776992-c11ad181-cb8b-4810-b439-08c4cb986d57.png)<br>
α、β、γi（_个人见解这里的yi是对应特征提取函数的l2距离前的_）是控制每个分量权重的超参数。超参数λ的选择非常重要的。对于特征f，浅层特征则选择一个相对较大的λ值，深度低层特征选择一个较小的λ值。
### 线性特征流形
流形学习的观点是认为，我们所能观察到的数据实际上是由一个低维流形映射到高维空间上的。由于数据内部特征的限制，一些高维中的数据会产生维度上的冗余，实际上只需要比较低的维度就能唯一地表示。
也就是说，流形可以作为一种数据降维的方式；另外流形能够刻画数据的本质。也就是说。既然学习到了“将数据从高维空间降维到低维空间，还能不损失信息”的映射，那这个映射能够输入原始数据，输出数据更本质的特征。<br>
<br>
假定一个d维线性特征流形M ⊂ R^d 嵌入在 R^m中，可定义一个构造函数h：
![image](https://user-images.githubusercontent.com/51207072/226779658-cfad7a3e-97cf-43b3-bb73-8e01507b3492.png)<br>
给定N个从DNN学习到的特征fi，可能是采样来自d维线性特征流，并且带有噪音：
![image](https://user-images.githubusercontent.com/51207072/226780118-33cd467c-c730-4bfd-ad60-f8fce1dff20b.png)<br>
vi是第i个特征fi的特征流形，ϵi是误差。目标是根据fi估计未知的低维特征向量vi，即想用更低维的vi来表示学习特征。<br>
![image](https://user-images.githubusercontent.com/51207072/226789547-949ebe4f-d07a-4fa8-9971-8ca700e5f89c.png)
F = [f1，f2···，fN ]包括所有N特性，我们的目标是找到c，A和V最小化近似误差E V = [v1，v2，···，vN]，E=[ϵ1，ϵ2，···ϵN],e是所有的N维列向量。
关于这里论证特征流形的举例二维的例子这里不再介绍,这里假设了c = f ¯ = F e/N（F的平均值），并采用SVD分解的方法来求AV的表达式，结论如下：<br>
![image](https://user-images.githubusercontent.com/51207072/226790110-0e90594c-ea88-4732-bde3-02a09b33878c.png)<br>
![image](https://user-images.githubusercontent.com/51207072/226790132-466bd384-eabc-497a-bed6-fff9c6bacb12.png)<br>

### 深层特征的特征流形
深度学习中特征流形更加复杂，非线性化。非线性流形学习的关键困难是无组织的降维数据。论文认为对非线性深度特征的流形学习的目标是在不明确知道h（·）的情况下，从相应的特征fi或h（vi）重构vi。

## 总结

# **Day 11 Catastrophic Interference in Reinforcement Learning: A Solution Based on Context Division and Knowledge Distillation**

## 论文摘要概述
强化学习（RL）代理能够直接从连续的环境中学习有能力的控制策略，但是在实际情况中中，训练数据是时间相关的和不平稳的，一般的RL范式无法保证相同和独立分布，这就影响了RL学习到的概率分布的有效性和预测能力。论文提出干扰感知的深度q学习（IQ）来减轻单任务深度RL中的灾难性干扰，具体采用在线聚类来实现动态上下文划分，以及多头网络和知识蒸馏正则化术语，以保存学习上下文的策略。
## 强化学习干扰问题
强化学习的RL问题实质上是是在神经网络中观察到的一种现象，未来的训练可能会覆盖和干扰之前学习到的良好策略，并显著降低之前任务的表现，而强化学习的数据不平稳放大了这种现象。如下图所示：<br>
![image](https://user-images.githubusercontent.com/51207072/226833719-1985d9e2-0dd0-4e1b-802d-1a7f7d16d5a0.png)<br>
- (a)是单任务RL中的灾难性干扰的说明。学习过程中数据分布的漂移，其中P1-P3是不同的数据分布，➀-➁代表分布转换。在学习过程中，代理会经历以下数据分布转换：序号表示迁移顺序，箭头表示迁移方向。
- (b)DNNs的稳定性-可塑性权衡。Sharing：这两个学习阶段(Old learning at P1 and Current learning at P2)训练相同的模型。transfer：当前的学习阶段继续对来自旧的学习阶段的模型进行训练。干扰：模型在P2上进行训练后，右侧网络中绿色的权值发生变化，影响了模型在P1上的性能。
- (c)学习曲线中，实线对应于(a)中的数据分布转换，虚线表示训练性能。在t3之前，数据分布逐渐从P1漂移到P2和P3。当模型适合于P3时，P1和P2上的学习策略就会受到干扰，当代理再次遇到来自P1的状态时，就会导致灾难性的性能下降。因此，该模型需要在P1上进行再训练（在T3→T4时间段内）。同样的问题也发生在时间段T4→T5中。
一旦训练数据的分布出现明显的漂移，就有可能发生灾难性的干扰和连锁反应，导致训练性能的突然恶化.
### 目前处理灾难性干扰的方案
- 经验重放，通常对关键参数（例如，重放缓冲区容量）表现出高水平的敏感性，并且通常需要保持一个大的临时存储内存。但是违背了当前最先进的算法的要求，即数据应该接近策略上的分布。
- 局部优化，提倡对具有特定分布的数据进行局部网络更新，而不是进行全局泛化，以减少不同数据分布之间的表示重叠。主要的问题是，一些方法在不同分布的数据之间的模型传输能力有限，或需要预训练，可能不适用于在线设置。
### 本文工作重点
本文主要讨论了单任务RL中由状态分布漂移引起的灾难性干扰问题。
- 提出一个干扰感知方案低缓冲大小灵敏度称为干扰感知深度Q-learing（IQ）
- 在线估计值函数为每个状态分布通过最小化原始损失函数的加权和RL算法和正规化术语关于不同状态之间的干扰
本文是通过如下方法来实现这些的：
- 提出了一种基于在线聚类（K-mean）的上下文划分策略，将状态空间划分为一系列独立的上下文，来减轻减轻模型训练过程中不同状态分布之间的干扰。
- 通过在多任务学习中常用的具有多个输出头的神经网络来参数化值函数，其中每个输出头专门针对特定的上下文，并且特征提取器在所有上下文中共享。
- 我们在目标函数估计值函数中应用知识蒸馏作为正则化项，可以保留学习到的策略，而RL代理在当前策略的指导下进行训练，以进一步避免共享的低级表示造成的干扰。
## 相关工作

### 上下文检测和识别
CL学习任务相关性需要界定任务边界，大多数多任务CL方法依赖于定义的任务边界，通常在一系列已知标签或边界的任务上进行训练。
<br>现有的上下文检测方法通常利用统计数据或贝叶斯推理来识别任务边界。
一方面，一些方法通过发现变化点模式状态奖励元组，跟踪短期和长期移动平均奖励，或将游戏分离成使用不打折扣的累积的游戏分数作为任务的上下文来响应变化的分布，这些方法可以敏捷地响应上下文或任务之间发生突然变化的场景，但是对平滑过渡的上下文并不敏感；另一方面，一些更雄心勃勃的方法试图直接从环境互动的历史中学习对未观察到的环境状态的信念，然而在应用于CL问题之前，它们都需要对完整的数据进行预训练，

## 一些定义

### RL范式
RL问题被认为是一个马尔可夫决策过程（MDP），被定义为一个元组：![image](https://user-images.githubusercontent.com/51207072/227400539-d1f33b61-b514-4540-8042-d4c91ef4b6b9.png)
，S是状态集，A是动作集；P：S×A×S→[0,1]是环境转移概率函数，R： S×A×S→R是奖励函数，γ∈[0,1]是折扣因子。<br>
根据定义，在每个时间步t∈N，代理（_the agent_）在采取At行动后以概率P（St+1|St，At）从St移动到St+1，并获得奖励R（St，At）。
在这个基础上定义了基于价值的RL模型的优化目标：
### RL优化目标



