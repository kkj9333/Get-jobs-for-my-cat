# **Day 10 An Efficient and Robust Cloud-based DeepLearning with Knowledge Distillation**

## 论文摘要概述
这篇论文在云边知识蒸馏领域提出了一种新的神经元流形蒸馏（NMD）方法，其中学生模型模拟教师的输出分布，并学习教师模型的特征几何。此外，为了进一步提高基于云的学习系统的可靠性，我们提出了一种自信的预测机制来校准模型的预测，并在多个数据集上证明了其有效性。
## 论文框架模型
论文提出的框架模型如下所示:<br>
![image](https://user-images.githubusercontent.com/51207072/226775333-3b4a23c5-5a3d-4efb-bcd2-6e2abe80c484.png)<br>
浅蓝色的教师网络是一个标准的深度残差网络ResNet110，奶油色的学生网络是一个浅层网络ResNet20，
两个网络都有3个**残余块**（residual block），但基本块的数量不同。ResNet110在每个残差块中有18个基本块，但ResNet20只有3个。知识蒸馏发生在每个**残余块**之后。
## 蒸馏方法NMD
在假定相同批次处理数据的知识蒸馏训练中，有教师特征集ft∈Ft和学生特征集fs∈Fs，对于预定义特征提取函数ψ(.)可以提取为ψ(ft)和ψ(fs)（注意是将高维特征映射到一个保留关键特征特征的相同维度的低维空间），于是有每对的l2距离：![image](https://user-images.githubusercontent.com/51207072/226776901-037d526c-7ae6-4c79-95c7-41fa53cca95d.png)，于是根据以前的蒸馏方法以及新的这个特征函数有新的损失函数：<br>
![image](https://user-images.githubusercontent.com/51207072/226776992-c11ad181-cb8b-4810-b439-08c4cb986d57.png)<br>
α、β、γi（_个人见解这里的yi是对应特征提取函数的l2距离前的_）是控制每个分量权重的超参数。超参数λ的选择非常重要的。对于特征f，浅层特征则选择一个相对较大的λ值，深度低层特征选择一个较小的λ值。
### 线性特征流形
流形学习的观点是认为，我们所能观察到的数据实际上是由一个低维流形映射到高维空间上的。由于数据内部特征的限制，一些高维中的数据会产生维度上的冗余，实际上只需要比较低的维度就能唯一地表示。
也就是说，流形可以作为一种数据降维的方式；另外流形能够刻画数据的本质。也就是说。既然学习到了“将数据从高维空间降维到低维空间，还能不损失信息”的映射，那这个映射能够输入原始数据，输出数据更本质的特征。<br>
<br>
假定一个d维线性特征流形M ⊂ R^d 嵌入在 R^m中，可定义一个构造函数h：
![image](https://user-images.githubusercontent.com/51207072/226779658-cfad7a3e-97cf-43b3-bb73-8e01507b3492.png)<br>
给定N个从DNN学习到的特征fi，可能是采样来自d维线性特征流，并且带有噪音：
![image](https://user-images.githubusercontent.com/51207072/226780118-33cd467c-c730-4bfd-ad60-f8fce1dff20b.png)<br>
vi是第i个特征fi的特征流形，ϵi是误差。目标是根据fi估计未知的低维特征向量vi，即想用更低维的vi来表示学习特征。<br>
![image](https://user-images.githubusercontent.com/51207072/226789547-949ebe4f-d07a-4fa8-9971-8ca700e5f89c.png)
F = [f1，f2···，fN ]包括所有N特性，我们的目标是找到c，A和V最小化近似误差E V = [v1，v2，···，vN]，E=[ϵ1，ϵ2，···ϵN],e是所有的N维列向量。
关于这里论证特征流形的举例二维的例子这里不再介绍,这里假设了c = f ¯ = F e/N（F的平均值），并采用SVD分解的方法来求AV的表达式，结论如下：<br>
![image](https://user-images.githubusercontent.com/51207072/226790110-0e90594c-ea88-4732-bde3-02a09b33878c.png)<br>
![image](https://user-images.githubusercontent.com/51207072/226790132-466bd384-eabc-497a-bed6-fff9c6bacb12.png)<br>

### 深层特征的特征流形
深度学习中特征流形更加复杂，非线性化。非线性流形学习的关键困难是无组织的降维数据。论文认为对非线性深度特征的流形学习的目标是在不明确知道h（·）的情况下，从相应的特征fi或h（vi）重构vi。

## 总结

# **Day 11 Catastrophic Interference in Reinforcement Learning: A Solution Based on Context Division and Knowledge Distillation**

## 论文摘要概述
强化学习（RL）代理能够直接从连续的环境中学习有能力的控制策略，但是在实际情况中中，训练数据是时间相关的和不平稳的，一般的RL范式无法保证相同和独立分布，这就影响了RL学习到的概率分布的有效性和预测能力。论文提出干扰感知的深度q学习（IQ）来减轻单任务深度RL中的灾难性干扰，具体采用在线聚类来实现动态上下文划分，以及多头网络和知识蒸馏正则化术语，以保存学习上下文的策略。
## 强化学习干扰问题
强化学习的RL问题实质上是是在神经网络中观察到的一种现象，未来的训练可能会覆盖和干扰之前学习到的良好策略，并显著降低之前任务的表现，而强化学习的数据不平稳放大了这种现象。如下图所示：<br>
![image](https://user-images.githubusercontent.com/51207072/226833719-1985d9e2-0dd0-4e1b-802d-1a7f7d16d5a0.png)<br>
- (a)是单任务RL中的灾难性干扰的说明。学习过程中数据分布的漂移，其中P1-P3是不同的数据分布，➀-➁代表分布转换。在学习过程中，代理会经历以下数据分布转换：序号表示迁移顺序，箭头表示迁移方向。
- (b)DNNs的稳定性-可塑性权衡。Sharing：这两个学习阶段(Old learning at P1 and Current learning at P2)训练相同的模型。transfer：当前的学习阶段继续对来自旧的学习阶段的模型进行训练。干扰：模型在P2上进行训练后，右侧网络中绿色的权值发生变化，影响了模型在P1上的性能。
- (c)学习曲线中，实线对应于(a)中的数据分布转换，虚线表示训练性能。在t3之前，数据分布逐渐从P1漂移到P2和P3。当模型适合于P3时，P1和P2上的学习策略就会受到干扰，当代理再次遇到来自P1的状态时，就会导致灾难性的性能下降。因此，该模型需要在P1上进行再训练（在T3→T4时间段内）。同样的问题也发生在时间段T4→T5中。
一旦训练数据的分布出现明显的漂移，就有可能发生灾难性的干扰和连锁反应，导致训练性能的突然恶化.
### 目前处理灾难性干扰的方案
- 经验重放，通常对关键参数（例如，重放缓冲区容量）表现出高水平的敏感性，并且通常需要保持一个大的临时存储内存。但是违背了当前最先进的算法的要求，即数据应该接近策略上的分布。
- 局部优化，提倡对具有特定分布的数据进行局部网络更新，而不是进行全局泛化，以减少不同数据分布之间的表示重叠。主要的问题是，一些方法在不同分布的数据之间的模型传输能力有限，或需要预训练，可能不适用于在线设置。
### 本文工作重点
本文主要讨论了单任务RL中由状态分布漂移引起的灾难性干扰问题。
- 提出一个干扰感知方案低缓冲大小灵敏度称为干扰感知深度Q-learing（IQ）
- 在线估计值函数为每个状态分布通过最小化原始损失函数的加权和RL算法和正规化术语关于不同状态之间的干扰
本文是通过如下方法来实现这些的：
- 提出了一种基于在线聚类（K-mean）的上下文划分策略，将状态空间划分为一系列独立的上下文，来减轻减轻模型训练过程中不同状态分布之间的干扰。
- 通过在多任务学习中常用的具有多个输出头的神经网络来参数化值函数，其中每个输出头专门针对特定的上下文，并且特征提取器在所有上下文中共享。
- 我们在目标函数估计值函数中应用知识蒸馏作为正则化项，可以保留学习到的策略，而RL代理在当前策略的指导下进行训练，以进一步避免共享的低级表示造成的干扰。
## 相关工作

### 上下文检测和识别
CL学习任务相关性需要界定任务边界，大多数多任务CL方法依赖于定义的任务边界，通常在一系列已知标签或边界的任务上进行训练。
<br>现有的上下文检测方法通常利用统计数据或贝叶斯推理来识别任务边界。
一方面，一些方法通过发现变化点模式状态奖励元组，跟踪短期和长期移动平均奖励，或将游戏分离成使用未折现的累积的游戏分数作为任务的上下文来响应变化的分布，这些方法可以敏捷地响应上下文或任务之间发生突然变化的场景，但是对平滑过渡的上下文并不敏感；另一方面，一些更雄心勃勃的方法试图直接从环境互动的历史中学习对未观察到的环境状态的信念，然而在应用于CL问题之前，它们都需要对完整的数据进行预训练，

## 一些定义&前置知识

### Huber loss
![image](https://user-images.githubusercontent.com/51207072/227679124-0a3012c2-92ea-46fe-ab68-325f368c54d7.png)


### RL范式（RL Paradigm）
RL问题被认为是一个马尔可夫决策过程（MDP），被定义为一个元组：![image](https://user-images.githubusercontent.com/51207072/227400539-d1f33b61-b514-4540-8042-d4c91ef4b6b9.png)
，S是状态集，A是动作集；P：S×A×S→[0,1]是环境转移概率函数，R： S×A×S→R是奖励函数，γ∈[0,1]是折扣因子。<br>
根据定义，在每个时间步t∈N，代理（_the agent_）在采取At行动后以概率P（St+1|St，At）从St移动到St+1，并获得奖励R（St，At）。
在这个基础上定义了基于价值的RL模型的优化目标：
### RL优化目标（RL Optimization Objective）
基于价值的RL的优化目标是学习一个内部参数为![image](https://user-images.githubusercontent.com/51207072/227401926-9db40931-ed3a-4492-bcb5-b39c9eec2253.png)
的策略π(a|s)，使每个(s, a)在时间上的预期长期折现收益最大化、 也被称为价值函数：<br>
![image](https://user-images.githubusercontent.com/51207072/227404186-804ca206-089b-48a2-99e6-5b9f6d2d19da.png)
在这里，期望值超过了使用P生成历史记录并决定从π到代理生命周期结束的操作的过程。这个优化目标不仅关注当前状态，还关注状态的完整预期未来分布。因此，**有可能**（作者yy）克服RL在非平稳数据分布上的灾难性干扰。<br>
然而，最近在RL方面的大部分工作都是在所谓的情景环境中，它优化了情景（_episodic_）RL的目标：
### 情景式RL优化目标（Episodic RL Optimization Objective）
给定某个未来期（future horizon）H，找到一个策略π(a|s)，优化预期折现收益：<br>
![image](https://user-images.githubusercontent.com/51207072/227404906-ad6b3dbf-00e0-4ebe-ab9c-6e9644025e8e.png)<br>
在这里，为了确保优化的可行性和易于实施，目标只在未来期H范围内进行优化，直到当前阶段结束。（作者假定）很明显，这个情景目标偏向于当前的情景分布，而忽略了在代理的一生中可能更重要的未来情景分布。这样的优化目标插入非平稳的RL设置中会导致偏置优化，这可能会导致灾难性的干扰效应。<br>
### 价值函数近似方法
对于大规模域，价值函数通常与参数函数类的成员近似，例如参数![image](https://user-images.githubusercontent.com/51207072/227407791-548270b4-87ba-4ff9-b49b-a72ecf551b83.png)，表示为Q(s,a;θ)的神经网络，使用以(s,a,r,s')形式的经验样本在线拟合。这种经验通常被收集到一个缓冲区B，然后随机抽取批次，以形成对损失（_loss_）的随机估计:<br>
![image](https://user-images.githubusercontent.com/51207072/227408249-b968e186-42ac-46ca-b43d-fb81337ce9c7.png)<br>
其中，L： ![image](https://user-images.githubusercontent.com/51207072/227410623-ee744878-5e81-40fd-b866-b7d2e9764575.png)是代理的损失函数，μ∈P (B)是定义其采样策略的分布。一般来说，用于计算目标Q(s',a';θ-)的参数θ−是用于动作选择的优先副本（作为DQN的设置）。
### 非平稳（ Nonstationary）
它是一个状态或概率分布随时间变化的过程。
### 干扰（Interference）
它是一种基于梯度的两个进程之间的影响，其目标是J1和J2，并共享参数θ。 干扰的特征通常是在一阶中由它们的梯度的内积来描述的：<br>
![image](https://user-images.githubusercontent.com/51207072/227413877-ea20ef72-0b5c-4b6b-9979-82956d5ccb3d.png)<br>
当梯度更新时（![image](https://user-images.githubusercontent.com/51207072/227414530-bcb0dae3-9331-4b79-a4e3-22e9d9c65443.png)）：<br>
- ρ > 0，转移，视为建设性的
- ρ < 0，干扰，视为破坏性的
### 灾难性干扰（Catastrophic Interference）
在神经网络训练中观察到的一种现象，学习一个新任务会显著降低先前任务的表现。
## 问题陈述
单任务RL内的干扰可以通过当前策略下模型更新前后的**TD误差**的差异来近似衡量，称为近似期望干扰（AEI）：<br>
![image](https://user-images.githubusercontent.com/51207072/227415288-7c2c6d4f-0335-4627-bfe7-99b912caee99.png)<br>
其中dˆ为当前策略下(s,a,r,s')的分布，TD误差是δ(s,a,r,s';θ)=r+![image](https://user-images.githubusercontent.com/51207072/227416080-fb12ea70-361a-47ea-ae66-11522fff89c0.png)-Q(s,a;θ)。
### 单任务RL训练中干扰和代理性能的相互作用
![image](https://user-images.githubusercontent.com/51207072/227427655-73423293-114f-4952-8e40-fed97b0d8aa7.png)
如图是OpenAI基线3中实现的DQN进行的实验，当AEI开始增加时，性能开始振荡，一般来说，在干扰增加的情况下，代理的性能倾向于显著下降。这一结果为干扰与单任务RL模型的稳定性密切相关提供了直接的证据。
### 本文解决问题表述
提出一种新的、有效的单任务RL训练方案，以减少训练过程中的灾难性干扰和性能振荡，同时提高稳定性和整体性能。
## 解决方案
本文的方案IQ由三个主要组成部分组成，共同优化以减轻单任务RL中的灾难性干扰：上下文划分、知识蒸馏和多头神经网络（采用了一个具有共享表示层的多头神经网络来为特定分布的价值函数设置参数）的协同训练。<br>
基于IQ，进一步提出了基于随机编码器（IQ-RE）的干扰感知深度q学习方法，用于高维状态空间的有效上下文化。
本文提出的IQ方案可以纳入任何现有的基于值的RL方法，以训练单任务RL的分段q函数。如图所示，该框架由三部分组成：
- 上下文划分，包括状态分配和质心更新（centroid update），自适应上下文划分通过顺序K-means在线聚类实现；
- 知识蒸馏-将知识蒸馏损失（LD）纳入目标函数（Lori），以避免共享特征提取器导致的上下文间干扰；
- 与多头神经网络联合优化，目的是通过联合优化损失（L = Lori +λLD）估计每个[s,a,w(s)]的值。
将神经网络的表示模块称为“RL编码器”。综上所述，我们的方法可以通过解耦不同分布状态之间的相关性和有意地保留学习到的策略来提高性能。<br>

![image](https://user-images.githubusercontent.com/51207072/227679852-6404b50c-f803-4faf-bf53-e2a9437487b8.png)

<br>
q函数的权值集合记为θ = {θS，θT，θF }，其中θS是一组共享参数，而θT和θF都是特定于上下文的参数： θT是对应于当前输入状态s的上下文，而θF是对应于其他输入状态s的上下文。本文以以IQ和基本的RL算法DQN的结合作为一个示例。
### 1.上下文划分
在MDPs中，状态（或“观察”）代表了关于环境的最全面的信息。为了理解不同分布的状态，为状态空间中相互接近的状态集合定义了一个变量ω，称为“上下文”：<br>
![image](https://user-images.githubusercontent.com/51207072/227680061-bf9232b3-11da-410b-9e78-e4ee51187b40.png)
<br>
其中Ω是一个有限的上下文集合，k是上下文的数量。对于任意MDP将其状态空间划分为k个上下文，每个上下文中的所有状态都遵循近似相同的分布，以解耦状态们与分布漂移的相关性，我们将一个上下文ωi与每个集合Si关联起来，因此对于s∈Si，ω(s) = ωi，其中ω(s)可以被认为是状态s的函数。<br>
由于RL学习中探索学习特性，代理（the agent）在寻找最优策略时通常不会体验到环境的所有可能状态。基于这个事实，在IQ中，我们只对训练中所经历的状态进行上下文划分。本文采用了顺序的K-means聚类来实现自适应的上下文检测。<br>
在如图框架中，k个质心C = {c1，c2，...，ck }在整个状态空间中被随机初始化。在随后的每个时间步长t中，我们对从环境接收到的每个传入状态执行状态分配和质心更新步骤（作者建议执行这两个步骤前在不同的维度上对状态进行规范化，以获得更合理的上下文划分结果。），并将其对应的转换（_transition_）{st,ω(st),at,rt,st+1,ω(st+1)}存储到重放缓冲区B（图中看起来像R）中。因此，在训练阶段，我们从B随机抽取一批转换，同时训练共享特征提取器![image](https://user-images.githubusercontent.com/51207072/227679940-ee5c766e-6979-44f6-88ad-cb75be9ee382.png)<br>
和输入状态对应的特定输出头![image](https://user-images.githubusercontent.com/51207072/227679953-abacc0a6-a933-4acb-b338-8602b68e6092.png)<br>
，同时对其他输出头进行微调（_Fine Tune_），以避免对学习策略的干扰。由于我们将每个状态的上下文标签存储在重播缓冲区中，因此在每个更新步骤中都不需要额外的状态分配（可以看到只需要更新w(s)）。（我们只需要为每个状态执行一次状态分配??）<br>
也可以基于初始状态分布进行上下文划分，相比之下，我们表明，对所有经历过的状态的分区在训练可以产生更准确和有效的上下文划分结果，轨迹从初始状态在不同的上下文中有很高的可能性重叠在后续的时间步骤。<br>

- 上下文干扰（Interference Among Contexts）。上下文划分法得到的上下文之间的干扰，具体来说，当代理在其他上下文下学习时，我们在游戏的不同上下文中测量TD错误的Huber损失，然后记录代理学习前后损失的相对变化。如上图结果表明，在任何情境下的长期训练都可能导致在所有其他情境下的负泛化，即使是在这样简单的RL任务CartPole-v0中。
![image](https://user-images.githubusercontent.com/51207072/227679972-b6bfe2cd-d7b4-4396-aee6-66bd0a5eb934.png)
通过在CartPole-v0上训练400k环境步骤时，聚类所有有经验过的状态来测量上下文之间的干扰（k = 3）。当代理在特定上下文上进行训练时，我们记录了所有上下文的Huber损失的相对变化。显然，对特定环境的培训通常会减少对其本身的损失，并增加对所有其他环境的损失。
- 计算复杂性（Computational Complexity）。假设有k个上下文的d维环境，我们提出的上下文划分模块处理T个环境步骤的时间和空间复杂度分别为O（Tkd）和O（kd）。
### 2.知识蒸馏












